{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7uYg7Vgg37SgZ4CHzUmm9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Regina-Arthur/Coding-Practice-Projects/blob/main/Papers_Explained/Lenet_Paper_Deep_Dive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Lenet Paper Deep Dive(*Gradient-based learning applied to document recognition*)**"
      ],
      "metadata": {
        "id": "yfQ1m17lgdo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Summary**\n",
        "\n"
      ],
      "metadata": {
        "id": "Yul_5-ofhSbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal of the paper**\n",
        "\n",
        "In the paper, they aimed to review and compare various methods applied to handwritten character recognition on a standard task.\n",
        "\n",
        "They wanted to show why the backpropagation algorithm is the best example of a successful gradient-based learning technique. They demonstrate that gradient-based learning, in general—whether using backpropagation for gradient calculation or not—can learn to classify data with a large number of factors determining the class (high-dimensional data) with minimal preprocessing.\n",
        "\n",
        "They also discuss the convolutional neural network, which is essentially a multilayer perceptron with convolutional layers added before it. These convolutional layers allow the model to perform its own feature extraction, identifying the features it deems necessary to achieve the desired results.\n",
        "\n",
        "**Why is minimal preprocessing of the data improtant?**\n",
        "\n",
        "Back then, whoever was designing the network had to preprocess the data by identifying, extracting, or creating the best features for the model to learn from before feeding them into the model. This process was time-consuming, and there was no way to verify whether the extracted or created features were optimal or reusable. Consequently, every time new data was introduced, features had to be extracted all over again. This is why this paper is valuable.\n",
        "\n",
        "**Why was feature extraction important in the first place?**\n",
        "\n",
        "That was because models at that time couldn't extract features themselves. Multilayer perceptrons (MLPs) could, but they had to be really large and trained over a long period of time. At the same time, it was difficult to optimize the parameters because there were so many. The higher the dimensionality of your data, the more parameters are needed to learn it accurately. When these parameters are incorrect, we need to adjust all of them repeatedly until we achieve the best results, which can be difficult and time-consuming.\n",
        "\n",
        "**Why couldn't models aside from the MLP extract feature?**\n",
        "\n",
        "Those models were quite simple and used basic mathematics, such as distance calculations, to classify the data. That is why mathematical equations also had to be designed to extract features for them.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "The terminologies at that time may differ from what they are today, even if they share the same name.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Gradient-based learning did not only represent the updating of weights using optimizers back then. It involved the entire process: the forward pass, loss calculation, backward pass (gradient calculation), and weight updates.\n",
        "\n",
        "Backpropagation was described as the most successful gradient-based learning technique in the paper, not merely because it is a gradient-based method itself, but because it was used for gradient calculation within the broader gradient-based learning process, with gradient descent applied for weight updates."
      ],
      "metadata": {
        "id": "VvucvqnGrPya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Introduction**"
      ],
      "metadata": {
        "id": "go4jEz8rldiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The introduction mainly explains why the feature extraction process should be automatically learned by the model in conjunction with the classification head.\n",
        "\n",
        "Models for the classification task were usually divided into two parts;\n",
        "  * The feature extractor\n",
        "  * The classification head\n",
        "\n",
        "During the building process, feature extraction was done manually by the practitioner. How well the feature extraction was performed depended entirely on the practitioner's skill in identifying the exact features the model needed to learn, which was often not optimal. The extracted features were also not reusable; they were specific to the data they were extracted from. The feature extraction process was tedious, so they argued that it should be made automatic, especially since it was mostly designed for low-dimensional data where the classes were relatively easy to separate.\n",
        "\n",
        "The reasons given for the switch were:\n",
        "  * The recent progress in machine learning has now provided techniques that allow models to automatically learn features on their own.\n",
        "\n",
        "  * The development of technology with higher processing and computational power has made the process easier, faster, and capable of producing better results.\n",
        "\n",
        "  * The increase in the size of datasets.\n"
      ],
      "metadata": {
        "id": "Bx0UzBqurbTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**A. Learning from Data**"
      ],
      "metadata": {
        "id": "lVrMqpGGx1N8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AgUazEk2x_YM"
      }
    }
  ]
}