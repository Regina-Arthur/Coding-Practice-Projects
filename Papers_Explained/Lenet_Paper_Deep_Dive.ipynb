{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSOXQ3YpS/RfZ8Ut1/LTxu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Regina-Arthur/Coding-Practice-Projects/blob/main/Papers_Explained/Lenet_Paper_Deep_Dive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Lenet Paper Deep Dive(*Gradient-based learning applied to document recognition*)**"
      ],
      "metadata": {
        "id": "yfQ1m17lgdo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Summary**\n",
        "\n",
        "**Goal of the paper**\n",
        "\n",
        "In the paper, they aimed to review and compare various methods applied to handwritten character recognition on a standard task.\n",
        "\n",
        "They wanted to show why the backpropagation algorithm is the best example of a successful gradient-based learning technique. They demonstrate that gradient-based learning, in general—whether using backpropagation for gradient calculation or not—can learn to classify data with a large number of factors determining the class (high-dimensional data) with minimal preprocessing.\n",
        "\n",
        "They also discuss the convolutional neural network, which is essentially a multilayer perceptron with convolutional layers added before it. These convolutional layers allow the model to perform its own feature extraction, identifying the features it deems necessary to achieve the desired results.\n",
        "\n",
        "**Why is minimal preprocessing of the data improtant?**\n",
        "\n",
        "Back then, whoever was designing the network had to preprocess the data by identifying, extracting, or creating the best features for the model to learn from before feeding them into the model. This process was time-consuming, and there was no way to verify whether the extracted or created features were optimal or reusable. Consequently, every time new data was introduced, features had to be extracted all over again. This is why this paper is valuable.\n",
        "\n",
        "**Why was feature extraction important in the first place?**\n",
        "\n",
        "That was because models at that time couldn't extract features themselves. Multilayer perceptrons (MLPs) could, but they had to be really large and trained over a long period of time. At the same time, it was difficult to optimize the parameters because there were so many. The higher the dimensionality of your data, the more parameters are needed to learn it accurately. When these parameters are incorrect, we need to adjust all of them repeatedly until we achieve the best results, which can be difficult and time-consuming.\n",
        "\n",
        "**Why couldn't models aside from the MLP extract feature?**\n",
        "\n",
        "Those models were quite simple and used basic mathematics, such as distance calculations, to classify the data. That is why mathematical equations also had to be designed to extract features for them.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "The terminologies at that time may differ from what they are today, even if they share the same name.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Gradient-based learning did not only represent the updating of weights using optimizers back then. It involved the entire process: the forward pass, loss calculation, backward pass (gradient calculation), and weight updates.\n",
        "\n",
        "Backpropagation was described as the most successful gradient-based learning technique in the paper, not merely because it is a gradient-based method itself, but because it was used for gradient calculation within the broader gradient-based learning process, with gradient descent applied for weight updates."
      ],
      "metadata": {
        "id": "Yul_5-ofhSbU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "go4jEz8rldiU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}