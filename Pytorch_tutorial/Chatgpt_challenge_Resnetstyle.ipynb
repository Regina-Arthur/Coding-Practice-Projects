{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPM4QE1L7Jib1wTodaWbB1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Regina-Arthur/Coding-Practice-Projects/blob/main/Pytorch_tutorial/Chatgpt_challenge_Resnetstyle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JkgJ08a2Sv7R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instructions from ChatGPT\n",
        "Nice — here’s a clean, step-by-step architecture you can build from scratch (no code, no pseudocode). I’ll pick a practical, modern, easy-to-train design: a compact ResNet-style CNN (ResNet-18 flavor) for image tasks. I give exact layer choices, channel counts, and how the spatial size changes so you can implement and verify shapes easily.\n",
        "\n",
        "If you want a different goal (object detection, segmentation, smaller/larger model) tell me and I’ll give an adjusted plan.\n",
        "\n",
        "Model: Compact ResNet-style CNN (good for classification)\n",
        "\n",
        "Input assumption: RGB image, 3 x 224 x 224.\n",
        "(If you use 448 x 448 or some other size, the same block layout works — only the spatial sizes after pooling change.)\n",
        "\n",
        "1) Stem (initial feature extractor)\n",
        "\n",
        "A convolution: kernel 7×7, filters 64, stride 2, padding 3.\n",
        "\n",
        "Purpose: quickly expand channels and reduce spatial size a bit.\n",
        "\n",
        "Batch Normalization on the 64 channels.\n",
        "\n",
        "ReLU activation.\n",
        "\n",
        "Max Pooling: 3×3 window, stride 2, padding 1.\n",
        "\n",
        "After the stem the spatial size for 224→ becomes 56×56.\n",
        "\n",
        "2) Residual stage 1 (output channels = 64)\n",
        "\n",
        "Repeat 2 residual blocks (standard 2-conv blocks):\n",
        "\n",
        "Each residual block contains:\n",
        "\n",
        "Conv 3×3, channels 64, stride 1, padding 1 → BN → ReLU\n",
        "\n",
        "Conv 3×3, channels 64, stride 1, padding 1 → BN\n",
        "\n",
        "Add (identity) → ReLU after addition\n",
        "\n",
        "No downsampling in this stage (identity skip is direct).\n",
        "\n",
        "After stage 1: spatial 56×56, channels 64.\n",
        "\n",
        "3) Residual stage 2 (output channels = 128)\n",
        "\n",
        "Repeat 2 residual blocks:\n",
        "\n",
        "First block in stage uses downsampling:\n",
        "\n",
        "The block’s first conv uses stride 2 (or use a 1×1 conv with stride 2 in the skip path) to halve spatial size.\n",
        "\n",
        "The identity/skip should also be projected (1×1 conv) to match channel count and stride.\n",
        "\n",
        "Subsequent blocks in the stage use stride 1.\n",
        "\n",
        "After stage 2: spatial 28×28, channels 128.\n",
        "\n",
        "4) Residual stage 3 (output channels = 256)\n",
        "\n",
        "Repeat 2 residual blocks:\n",
        "\n",
        "First block downsamples (stride 2) and projects skip to match 256 channels.\n",
        "\n",
        "Remaining blocks: stride 1.\n",
        "\n",
        "After stage 3: spatial 14×14, channels 256.\n",
        "\n",
        "5) Residual stage 4 (output channels = 512)\n",
        "\n",
        "Repeat 2 residual blocks:\n",
        "\n",
        "First block downsamples (stride 2) and projects skip to match 512 channels.\n",
        "\n",
        "Remaining blocks: stride 1.\n",
        "\n",
        "After stage 4: spatial 7×7, channels 512.\n",
        "\n",
        "(This stage/block counts are the ResNet-18 pattern: [2,2,2,2]. You can increase blocks per stage for a deeper model.)\n",
        "\n",
        "6) Classification head\n",
        "\n",
        "Global average pooling across the 7×7 spatial dims → produces 512 values per sample (one per channel).\n",
        "\n",
        "Optional dropout (e.g., 0.5) before the FC if you want more regularization.\n",
        "\n",
        "Fully connected (linear) layer to num_classes outputs.\n",
        "\n",
        "Use an appropriate final form for the task: softmax (for cross-entropy training on multi-class) — note: in practice you pass raw logits to a cross-entropy loss which applies softmax internally.\n",
        "\n",
        "7) Useful internals / standard practices (where to put BN / activations)\n",
        "\n",
        "Put BatchNorm after each convolution, before ReLU.\n",
        "\n",
        "Use ReLU for nonlinearity throughout (or LeakyReLU if you prefer).\n",
        "\n",
        "For shortcut projections (when channels/stride change) use a 1×1 conv + BN on the skip path.\n",
        "\n",
        "Weight initialization: He/Kaiming for ReLU (standard frameworks set sensible defaults).\n",
        "\n",
        "Shapes at each major milestone (for 224×224 input)\n",
        "\n",
        "Input: 3 × 224 × 224\n",
        "\n",
        "After stem conv (stride 2): 64 × 112 × 112\n",
        "\n",
        "After stem maxpool (stride 2): 64 × 56 × 56\n",
        "\n",
        "After stage1: 64 × 56 × 56\n",
        "\n",
        "After stage2: 128 × 28 × 28\n",
        "\n",
        "After stage3: 256 × 14 × 14\n",
        "\n",
        "After stage4: 512 × 7 × 7\n",
        "\n",
        "After global avg pool: 512\n",
        "\n",
        "After FC: num_classes\n",
        "\n",
        "(If input is 448×448 the final spatial dims will double at intermediate steps; adjust pooling to still get reasonable grid.)\n",
        "\n",
        "Variants & options (pick one)\n",
        "\n",
        "Smaller dataset / faster model: use fewer channels (32→64→128→256) and fewer blocks.\n",
        "\n",
        "Deeper model: change block counts to ResNet-34 or ResNet-50 (ResNet-50 uses bottleneck blocks: 1×1 reduce → 3×3 → 1×1 expand).\n",
        "\n",
        "If your task is detection (like YOLO): replace the classification head with a detection head that outputs a S×S×(B*5 + C) grid — the backbone (stages above) can be reused as the feature extractor.\n",
        "\n",
        "If you want stronger regularization: add dropout, stronger data augmentation, weight decay, label smoothing.\n",
        "\n",
        "Why this architecture?\n",
        "\n",
        "Residual connections let you train deeper networks reliably and are easy to implement.\n",
        "\n",
        "Doubling channels at each downsample balances representation capacity vs spatial resolution.\n",
        "\n",
        "Global average pooling replaces big fully connected layers, reduces params and overfitting.\n",
        "\n",
        "This architecture is a solid baseline: small-ish, trains well, and you can iterate (deeper/shallower) once you validate on data.\n",
        "\n",
        "If you want a different specific target (tiny CNN for mobile, a full YOLO-style detection backbone, or a U-Net segmentation encoder/decoder) tell me which task and dataset size and I’ll give a tailored architecture plan the same way."
      ],
      "metadata": {
        "id": "KXt2TZgDarhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__\n",
        "    self.layer1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, stride = 2, padding = 3)\n",
        "    self.batchnorm = nn.BatchNorm2d(num_features = 64)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pooling = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "    self.resblock = nn.Sequential((nn.Conv2d(in_channels = 64 , out_channels =64 , kernel_size = 3, stride = 1, padding = 1),\n",
        "                                  nn.BatchNorm2d(num_features = 64),\n",
        "                                  nn.ReLU()),\n",
        "                                  (nn.Conv2d(in_channels = 64 , out_channels =64 , kernel_size = 3, stride = 1, padding = 1),\n",
        "                                  nn.BatchNorm2d(num_features = 64),\n",
        "                                  #we need to put a skip connection here, so we will define a function for that and add it\n",
        "                                  nn.ReLU()))\n",
        "    self.resblock = nn.Sequential((nn.Conv2d(in_channels = 64 , out_channels =64 , kernel_size = 3, stride = 1, padding = 1),\n",
        "                                  nn.BatchNorm2d(num_features = 64),\n",
        "                                  nn.ReLU()),\n",
        "                                  (nn.Conv2d(in_channels = 64 , out_channels =64 , kernel_size = 3, stride = 1, padding = 1),\n",
        "                                  nn.BatchNorm2d(num_features = 64),\n",
        "                                  #we need to put a skip connection here, so we will define a function for that and add it\n",
        "                                  nn.ReLU()))"
      ],
      "metadata": {
        "id": "H_r-4CHhWOX4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}